name: Deploy to docs-content.oniudra.cc

on:
  workflow_dispatch:
  push:
    branches:
      - ghaction

concurrency:
  group: deploy-staging
  cancel-in-progress: true

# Allow installation of dependencies
permissions:
  id-token: write
  contents: read

jobs:
  #Â This job is used to render datasheets, but only if they have changed.
  # It's a separate job so we don't have to cleanup the machine afterwards.
  render-datasheets:
    name: Render Datasheets
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: ./.github/actions/generate-datasheets
        with:
          artifact-name: datasheets
          datasheets-path: static/resources/datasheets

  build:
    name: Build and Deploy
    needs: render-datasheets
    runs-on: ubuntu-latest
    environment: staging
    env:
      REPO_ACCESS_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }}
      APP_ENV: staging

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: "0"

      - name: Cleanup runner disk
        uses: ./.github/actions/cleanup-disk # Cleanup machine before starting the build

      - uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: "npm"
          cache-dependency-path: "package-lock.json"

      - name: Retrieve Datasheets
        uses: actions/download-artifact@v4 # Retrieve the datasheets generated in the previous job
        with:
          name: datasheets
          path: static/resources/datasheets

      - name: Debug datasheet list
        run: ls -lah static/resources/datasheets

      - name: Copy Static Files
        run: |
          mkdir -p static/resources/schematics static/resources/pinouts static/resources/models
          find ./content/hardware -type f -name "*-schematics.pdf" -exec cp {} ./static/resources/schematics/ \;
          find ./content/hardware -type f -name "*-full-pinout.pdf" -exec cp {} ./static/resources/pinouts/ \;
          find ./content/hardware -type f -name "*-pinout.png" -exec cp {} ./static/resources/pinouts/ \;
          find ./content/hardware -type f -name "*-step.zip" -exec cp {} ./static/resources/models/ \;

      - name: Check for large files
        run: |
          find content -type f \( -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.svg" -o -name "*.gif" \) -size +2M -print0 |
          while IFS= read -r -d $'\0' file; do
            size=$(ls -lh "$file" | awk '{print $5}')
            echo "::warning file=$file::File '$file' is very large: $size. Consider optimizing it."
          done

          find content -type f \( -name "*.pdf" -o -name "*.zip" -o -name "*.mp4" \) -size +10M -print0 |
          while IFS= read -r -d $'\0' file; do
            size=$(ls -lh "$file" | awk '{print $5}')
            echo "::warning file=$file::File '$file' is very large: $size. This can impact repository size and checkout times."
          done

      # - name: Fail on Oversized Files
      #   id: fail_on_oversized_files
      #   run: |
      #     #!/bin/bash
      #     # This script is a stricter check that fails the workflow if files exceed size limits.
      #     set -e

      #     # Thresholds in Megabytes (same as the warning step)
      #     IMAGE_LIMIT_MB=2
      #     ASSET_LIMIT_MB=10

      #     # Find oversized files and prepare a detailed report
      #     OVERSIZED_FILES=$(find content -type f \
      #       \( \( -name "*.png" -o -name "*.jpg" -o -name "*.jpeg" -o -name "*.svg" -o -name "*.gif" \) -size "+${IMAGE_LIMIT_MB}M" \) -o \
      #       \( \( -name "*.pdf" -o -name "*.zip" -o -name "*.mp4" \) -size "+${ASSET_LIMIT_MB}M" \) \
      #       -exec ls -lh {} + | awk '{printf "  - %s (%s)\n", $9, $5}')

      #     # Check if the report is non-empty and fail the job if so
      #     if [ -n "$OVERSIZED_FILES" ]; then
      #       echo "::error::Found files exceeding size limits. This new check is failing the build as intended."
      #       echo "--------------------------------------------------"
      #       echo "Oversized Files Found:"
      #       echo "$OVERSIZED_FILES"
      #       echo "--------------------------------------------------"
      #       echo "Limits:"
      #       echo "  - Images (PNG, JPG, SVG, GIF): ${IMAGE_LIMIT_MB}MB"
      #       echo "  - Assets (PDF, ZIP, MP4): ${ASSET_LIMIT_MB}MB"
      #       exit 1
      #     else
      #       echo "No oversized files found. New check passed."
      #     fi

      - name: Compress Large Images
        run: |
          sudo apt-get update && sudo apt-get install -y jpegoptim optipng
          echo "Compressing large JPGs..."
          find content -type f \( -name "*.jpg" -o -name "*.jpeg" \) -size +2M -print -exec jpegoptim --strip-all {} \;
          echo "Compressing large PNGs..."
          find content -type f -name "*.png" -size +2M -print -exec optipng -o2 {} \;

      - name: Gatsby main cache
        uses: actions/cache@v4
        id: gatsby-cache-folder
        with:
          path: .cache
          key: ${{ runner.os }}-cache-gatsby-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-cache-gatsby-main

      - name: Gatsby Public Folder cache
        uses: actions/cache@v4
        id: gatsby-public-folder
        with:
          path: public/
          key: ${{ runner.os }}-public-gatsby-${{ github.ref_name }}
          restore-keys: |
            ${{ runner.os }}-public-gatsby-main

      - run: npm install

      - name: Build
        env:
          NODE_OPTIONS: "--max-old-space-size=8192"
          GATSBY_HF_CDN_URL: ""
        run: npm run build

      - name: Clean up node_modules # Just to save space
        run: rm -rf node_modules

      - name: Deploy to S3
        uses: ./.github/actions/sync-s3
        with:
          role-to-assume: ${{ secrets.STAGING_IAM_ROLE }}
          bucket-name: ${{ secrets.STAGING_BUCKET_NAME }}

  purge-datasheets:
    name: Purge Datasheets cache
    needs: build
    runs-on: ubuntu-latest
    environment: staging
    steps:
      - name: Purge Cloudflare Cache
        shell: bash
        run: |
          echo "Purging Cloudflare cache for prefix: ${{ vars.DATASHEETS_BASE_URL }}, zone: ${{ vars.CLOUDFLARE_ZONE }}"
          curl -f -X POST "https://api.cloudflare.com/client/v4/zones/${{ vars.CLOUDFLARE_ZONE }}/purge_cache" \
            -H "Authorization: Bearer ${{ secrets.CLOUDFLARE_PURGE_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            --data '{"prefixes":["${{ vars.DATASHEETS_BASE_URL }}"]}'
